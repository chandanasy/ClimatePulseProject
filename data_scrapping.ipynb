{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk7Y9trBIAS9"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO9XlSyJFWal",
        "outputId": "dc557bd2-1617-488d-aa59-8044d63f68d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google.cloud\n",
            "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
            "Installing collected packages: google.cloud\n",
            "Successfully installed google.cloud-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install google.cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTNOpr07vwqp",
        "outputId": "e342f9de-c5d4-4b16-8a1e-13c101c32b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.1/492.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.3.0 pymongo-4.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tnRMKFC0FOy",
        "outputId": "0c3f9743-6d7d-4040-d167-70814d3302e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=4a167565800f645c87997aee97b6ce39d940932b157ac8b89b30246d80b07f3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Mo6MJPIJzq"
      },
      "source": [
        "Connect MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_u9b4WKskit",
        "outputId": "255e1f3e-125a-4e9b-bb08-83cf2f98e783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "uri = \"use your MongoDB URI\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "mongo_client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    mongo_client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gko0IvqIOzD"
      },
      "source": [
        "Reddit Scrapping and storing it in Mongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq0ksiOxsrwk"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "from pymongo import MongoClient\n",
        "import datetime\n",
        "\n",
        "# Set up your Reddit API credentials\n",
        "client_id = 'insert your id'\n",
        "client_secret = 'insert key'\n",
        "user_agent = 'mybot'\n",
        "\n",
        "# Set up MongoDB connection\n",
        "db = mongo_client['reddit_db']\n",
        "collection = db['climate_change_posts']\n",
        "\n",
        "# Initialize the Reddit API wrapper\n",
        "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent, check_for_async=False)\n",
        "\n",
        "# Define the search parameters\n",
        "search_terms = [\n",
        "    \"climate change\", \"Global warming\", \"Greenhouse effect\", \"Carbon dioxide emissions\",\n",
        "    \"Renewable energy\", \"Sea level rise\", \"Climate adaptation\", \"Climate mitigation\",\n",
        "    \"Extreme weather events\", \"Deforestation\", \"Melting glaciers\", \"Ocean acidification\",\n",
        "    \"Climate policy\", \"Carbon footprint\", \"Sustainable development\", \"Fossil fuels\",\n",
        "    \"Climate\", \"Biodiversity loss\", \"Energy efficiency\", \"Climate action\"\n",
        "]\n",
        "search_results_limit = 100  # Number of search results to retrieve\n",
        "\n",
        "# Calculate the search timeframe (last week)\n",
        "current_date = datetime.datetime.now()\n",
        "week_ago_date = current_date - datetime.timedelta(days=7)\n",
        "\n",
        "# Store the search results and their comments in MongoDB\n",
        "for search_term in search_terms:\n",
        "    search_query = f'{search_term} timestamp:{week_ago_date.timestamp()}:'\n",
        "    search_results = reddit.subreddit('all').search(search_query, time_filter='week', limit=search_results_limit)\n",
        "\n",
        "    for post in search_results:\n",
        "        post_data = {\n",
        "            'title': post.title,\n",
        "            'score': post.score,\n",
        "            'url': post.url,\n",
        "            'author': post.author.name if post.author else None,\n",
        "            'created_utc': post.created_utc,\n",
        "            'comments': []\n",
        "        }\n",
        "        post.comments.replace_more(limit=10)\n",
        "        for comment in post.comments.list():\n",
        "            comment_data = {\n",
        "                'body': comment.body,\n",
        "                'score': comment.score,\n",
        "                'author': comment.author.name if comment.author else None,\n",
        "                'created_utc': comment.created_utc\n",
        "            }\n",
        "            post_data['comments'].append(comment_data)\n",
        "\n",
        "        collection.insert_one(post_data)\n",
        "\n",
        "print(\"Data successfully scraped and stored in MongoDB.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtTL_6ruIbL6"
      },
      "source": [
        "Retrieve from MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3_Ft6m_vsupH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Access the database and collection\n",
        "db = mongo_client['reddit_db']\n",
        "collection = db['climate_change_posts']\n",
        "\n",
        "# Fetch all documents from the collection\n",
        "documents = collection.find()\n",
        "\n",
        "# Initialize empty lists to store posts and comments data\n",
        "posts_data = []\n",
        "comments_data = []\n",
        "\n",
        "# Iterate over the documents\n",
        "for post in documents:\n",
        "    # Extract post data\n",
        "    post_data = {\n",
        "        'title': post['title'],\n",
        "        'score': post['score'],\n",
        "        'url': post['url'],\n",
        "        'author': post['author'] if post['author'] else None,\n",
        "        'created_utc': post['created_utc']\n",
        "    }\n",
        "\n",
        "    # Append post data to the posts_data list\n",
        "    posts_data.append(post_data)\n",
        "\n",
        "    # Iterate over the comments\n",
        "    for comment in post['comments']:\n",
        "        # Extract comment data\n",
        "        comment_data = {\n",
        "            'body': comment['body'],\n",
        "            'score': comment['score'],\n",
        "            'author': comment['author'] if comment['author'] else None,\n",
        "            'created_utc': comment['created_utc']\n",
        "        }\n",
        "\n",
        "        # Append comment data to the comments_data list\n",
        "        comments_data.append(comment_data)\n",
        "\n",
        "# Convert the lists of post and comment data to Pandas DataFrames\n",
        "posts_df = pd.DataFrame(posts_data)\n",
        "comments_df = pd.DataFrame(comments_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJHZ0-DiIjFr"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n7uRwgrV2Pag"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
        "\n",
        "# Create a SparkConf object and set the desired configuration properties\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.submit.deployMode\", \"client\")\n",
        "\n",
        "# Set the driver memory to 16 GB\n",
        "conf.set(\"spark.driver.memory\", \"16g\")\n",
        "\n",
        "# Create a SparkSession and SparkContext\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load the saved model\n",
        "lrModel = LogisticRegressionModel.load(\"/content/drive/MyDrive/model\")\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\")\n",
        "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "countVectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
        "\n",
        "spark_df = spark.createDataFrame(comments_df)\n",
        "data_final = spark_df.select(\"body\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    regexTokenizer,\n",
        "    stopWordsRemover,\n",
        "    countVectorizer,\n",
        "    label_stringIdx\n",
        "])\n",
        "\n",
        "pipelineFit = PipelineModel.load(\"/content/drive/MyDrive/pipeline\")\n",
        "dataset = pipelineFit.transform(data_final)\n",
        "\n",
        "# Make predictions using the logistic regression model\n",
        "predictions = lrModel.transform(dataset)\n",
        "\n",
        "# Select the 'body' and 'prediction' columns from the predictions\n",
        "body_predictions = predictions.select(\"body\", \"prediction\")\n",
        "\n",
        "# Convert the Spark DataFrame back to Pandas DataFrame\n",
        "body_predictions_pandas = body_predictions.toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gULD7EcDDRHA"
      },
      "outputs": [],
      "source": [
        "merged_df = comments_df.merge(body_predictions_pandas, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "merged_df = merged_df.drop('body_y', axis=1)\n",
        "merged_df = merged_df.drop('author', axis=1)\n",
        "# Remove duplicate values from 'body_x' column\n",
        "merged_df['body_x'] = merged_df['body_x'].drop_duplicates()\n",
        "\n",
        "# Rename 'body_x' column to 'body'\n",
        "merged_df = merged_df.rename(columns={'body_x': 'body'})\n",
        "\n",
        "merged_df = merged_df.drop_duplicates(subset='body', keep='first')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL1ufE8LImx6"
      },
      "source": [
        "Store it in BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v90f-B2PD2Bn",
        "outputId": "fb9fb351-9c3b-4681-8b12-cbf053f0bef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table created: famous-athlete-386604.reddit.comment\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"include json file with your Google cloud credentials\"\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Create a BigQuery client\n",
        "client = bigquery.Client()\n",
        "\n",
        "project_id = 'famous-athlete-386604'\n",
        "dataset_name = 'reddit'\n",
        "table_name = 'comment'\n",
        "\n",
        "# Create a BigQuery client\n",
        "bigquery_client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Create the dataset reference\n",
        "dataset_ref = bigquery_client.dataset(dataset_name)\n",
        "\n",
        "# Create the dataset if it doesn't exist\n",
        "if not bigquery_client.get_dataset(dataset_ref):\n",
        "    bigquery_client.create_dataset(dataset_ref)\n",
        "\n",
        "# Define the table schema\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"body\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"score\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"created_utc\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"prediction\", \"FLOAT\"),\n",
        "]\n",
        "\n",
        "# Create the table reference\n",
        "table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "print(f'Table created: {project_id}.{dataset_name}.{table_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HCEOtacFM9Y",
        "outputId": "13136270-0c02-463c-9a0f-182818256c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data uploaded to BigQuery table: famous-athlete-386604.reddit.comment\n"
          ]
        }
      ],
      "source": [
        "# Write the DataFrame to the BigQuery table\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job = bigquery_client.load_table_from_dataframe(merged_df, table_ref, job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f'Data uploaded to BigQuery table: {project_id}.{dataset_name}.{table_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KEbcWH7ZF10w"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
        "\n",
        "# Create a SparkConf object and set the desired configuration properties\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.submit.deployMode\", \"client\")\n",
        "\n",
        "# Set the driver memory to 16 GB\n",
        "conf.set(\"spark.driver.memory\", \"16g\")\n",
        "\n",
        "# Create a SparkSession and SparkContext\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load the saved model\n",
        "lrModel = LogisticRegressionModel.load(\"/content/drive/MyDrive/model\")\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol=\"sentiment_label\", outputCol=\"label\")\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\")\n",
        "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "countVectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
        "posts_df = posts_df.rename(columns={'title': 'body'})\n",
        "spark_df = spark.createDataFrame(posts_df)\n",
        "data_final = spark_df.select(\"body\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    regexTokenizer,\n",
        "    stopWordsRemover,\n",
        "    countVectorizer,\n",
        "    label_stringIdx\n",
        "])\n",
        "\n",
        "pipelineFit = PipelineModel.load(\"/content/drive/MyDrive/pipeline\")\n",
        "dataset = pipelineFit.transform(data_final)\n",
        "\n",
        "# Make predictions using the logistic regression model\n",
        "predictions = lrModel.transform(dataset)\n",
        "\n",
        "# Select the 'body' and 'prediction' columns from the predictions\n",
        "body_predictions = predictions.select(\"body\", \"prediction\")\n",
        "\n",
        "# Convert the Spark DataFrame back to Pandas DataFrame\n",
        "body_predictions_pandas = body_predictions.toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "L5khvVj-F8gD"
      },
      "outputs": [],
      "source": [
        "merged_df = posts_df.merge(body_predictions_pandas, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "merged_df = merged_df.drop('body_y', axis=1)\n",
        "merged_df = merged_df.drop('author', axis=1)\n",
        "merged_df = merged_df.drop('url', axis=1)\n",
        "# Remove duplicate values from 'body_x' column\n",
        "merged_df['body_x'] = merged_df['body_x'].drop_duplicates()\n",
        "\n",
        "# Rename 'body_x' column to 'body'\n",
        "merged_df = merged_df.rename(columns={'body_x': 'body'})\n",
        "\n",
        "merged_df = merged_df.drop_duplicates(subset='body', keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00okD-svGHcK",
        "outputId": "08123986-4f11-4a4b-fd7f-b0c3e7fa17e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table created: famous-athlete-386604.reddit.comment\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"replace with your credentials\"\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Create a BigQuery client\n",
        "client = bigquery.Client()\n",
        "\n",
        "project_id = 'replace with your project id'\n",
        "dataset_name = 'reddit'\n",
        "table_name = 'comment'\n",
        "\n",
        "# Create a BigQuery client\n",
        "bigquery_client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Create the dataset reference\n",
        "dataset_ref = bigquery_client.dataset(dataset_name)\n",
        "\n",
        "# Create the dataset if it doesn't exist\n",
        "if not bigquery_client.get_dataset(dataset_ref):\n",
        "    bigquery_client.create_dataset(dataset_ref)\n",
        "\n",
        "# Define the table schema\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"body\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"score\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"created_utc\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"prediction\", \"FLOAT\"),\n",
        "]\n",
        "\n",
        "# Create the table reference\n",
        "table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "print(f'Table created: {project_id}.{dataset_name}.{table_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejSRQTKtHdJz",
        "outputId": "c6af9b36-47e2-4340-be86-3cc8d81e9603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data uploaded to BigQuery table: famous-athlete-386604.reddit.comment\n"
          ]
        }
      ],
      "source": [
        "# Write the DataFrame to the BigQuery table\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job = bigquery_client.load_table_from_dataframe(merged_df, table_ref, job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f'Data uploaded to BigQuery table: {project_id}.{dataset_name}.{table_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UM0XDz8I_Zp"
      },
      "source": [
        "Dropping the Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8mV2hUXJDDe",
        "outputId": "52eae402-9dbe-4677-c6bb-0c1f13afb931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection dropped successfully.\n"
          ]
        }
      ],
      "source": [
        "# Access the database and collection\n",
        "db = mongo_client['reddit_db']\n",
        "collection = db['climate_change_posts']\n",
        "\n",
        "# Drop the collection\n",
        "collection.drop()\n",
        "\n",
        "# Confirm if the collection has been dropped\n",
        "collections_list = db.list_collection_names()\n",
        "if 'climate_change_posts' in collections_list:\n",
        "    print(\"Collection was not dropped successfully.\")\n",
        "else:\n",
        "    print(\"Collection dropped successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
