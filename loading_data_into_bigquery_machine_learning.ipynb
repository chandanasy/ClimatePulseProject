{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqsDc2p-eb4t",
        "outputId": "202c1c41-6e9a-430e-dcb4-459745562718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=bd50fa55fabfce659e83fe67bc75fb787ccc65f2af3527ac56b8c0cbf0505c14\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GibLyAtRI3H6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"replace with your credentials\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3XFpg-MSI4FH"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create a BigQuery client\n",
        "client = bigquery.Client()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9l1uhnaI6fN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "# Set the GCS bucket and CSV file path\n",
        "bucket_name = 'reddit_data_big'\n",
        "file_path = 'the-reddit-climate-change-dataset-comments.csv'\n",
        "\n",
        "# Create a GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Retrieve the CSV file from GCS\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "\n",
        "# Download the CSV file to a temporary local file\n",
        "temp_file = '/tmp/temp_file.csv'\n",
        "blob.download_to_filename(temp_file)\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(temp_file)\n",
        "# Set up BigQuery client and dataset reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g1wAp3oecB-"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK03tzMSlFcT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession   \n",
        "from pyspark import SparkContext, SparkConf\n",
        "cf = SparkConf()\n",
        "cf.set(\"spark.submit.deployMode\", \"client\")\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext.getOrCreate(cf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAUd3e90lXAf"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('senti').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1H-Uw0IecUc"
      },
      "outputs": [],
      "source": [
        "customSchema = StructType([\n",
        "    StructField(\"body\", StringType()), \n",
        "    StructField(\"sentiment\", FloatType())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1hfHQ_R3k9e3"
      },
      "outputs": [],
      "source": [
        "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(temp_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZFzZ52KceqgC"
      },
      "outputs": [],
      "source": [
        "data = df1.na.drop(how='any')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQJQYkLveqkt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "data_with_sentiment = data.withColumn(\"sentiment_label\", \n",
        "                                      when(data.sentiment > 0, 0).otherwise(\n",
        "                                      when(data.sentiment < 0, 1).otherwise(2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLCgPKUgeqqM"
      },
      "outputs": [],
      "source": [
        "data_final = data_with_sentiment[\"body\",\"sentiment_label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD_GnMmmJfNU"
      },
      "source": [
        "##*Tokenizing and Cleaning the Data*\n",
        "\n",
        "The RegexTokenizer breaks down the string into an array of tokens. This is followed by removing sthe topwords (words without any meaning) using the StopWordsRemover from the spark.ml.feature library. CountVectorizer takes the output from the StopWordsRemover as input and generates a vocabulary of the most frequent words that occur in the text documents. It then maps each document to a vector of word counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muNMEdpLe6Uo"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# regular expression tokenizer\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# stop words\n",
        "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "\n",
        "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "\n",
        "# bag of words count\n",
        "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkc7Q-aeJixE"
      },
      "source": [
        "##*Defining the Pipeline*\n",
        "This block of code creates a data processing pipeline using the previously defined regexTokenizer, stopwordsRemover, countVectors, and label_stringIdx stages.\n",
        "\n",
        "The StringIndexer stage is used to encode the target variable sentiment_label as a numeric label in a new column called label.\n",
        "\n",
        "The pipeline is then fit to the training data data_final using the fit method, and the resulting pipeline model is used to transform the data using the transform method, which applies the processing stages to the data and creates a new column features containing the bag-of-words representation of the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "2Zyfyz8Ue6YB",
        "outputId": "12aac6a5-0e71-437e-cba3-4ab43d3d5bea"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"sentiment_label\", outputCol = \"label\")\n",
        "\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(data_final)\n",
        "dataset = pipelineFit.transform(data_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP_tbfYD7ppB"
      },
      "outputs": [],
      "source": [
        "pipelineFit.save(\"pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR7NZ1b-Jtlc"
      },
      "source": [
        "##*Splitting of the historical Dataset into Train and Test data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMoCbqgre6ap",
        "outputId": "fa74daf7-f271-4141-ec47-acbf9b92e379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 765907\n",
            "Test Dataset Count: 327969\n"
          ]
        }
      ],
      "source": [
        "# set seed for reproducibility\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTdeIpXiJx43"
      },
      "source": [
        "##*Defining the model for Predicition of Sentiments*\n",
        "This code block is fitting a logistic regression model to the training data and making predictions on the test data using the trained model.\n",
        "\n",
        "First, a LogisticRegression object is created with some specified hyperparameters (maxIter, regParam, and elasticNetParam).\n",
        "\n",
        "Next, the fit() method of the LogisticRegression object is called with the training data as input, which trains the logistic regression model on the training data. The resulting trained model is stored in the lrModel variable.\n",
        "\n",
        "Finally, the trained model is used to make predictions on the test data using the transform() method. The resulting predictions are stored in the predictions variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-GDA2Zie6gH"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(maxIter=100, regParam=0.5, elasticNetParam=0)\n",
        "\n",
        "#prepare data \n",
        "\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFEPoNS56V6n"
      },
      "outputs": [],
      "source": [
        "lrModel.save(\"model\")\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions = lrModel.transform(testData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAfe1YFiuYRP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Create a SparkConf object and set the desired configuration properties\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.submit.deployMode\", \"client\")\n",
        "\n",
        "# Set the driver memory to 16 GB\n",
        "conf.set(\"spark.driver.memory\", \"16g\")\n",
        "\n",
        "# Create a SparkSession and SparkContext\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQxWSMvyfFME"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(temp_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmZvKGfd3LF4"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'subreddit.nsfw': 'snsfw'}, inplace=True)\n",
        "df.rename(columns={'subreddit.id': 'sid'}, inplace=True)\n",
        "df.rename(columns={'subreddit.name': 'sname'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk2czt4S01di"
      },
      "outputs": [],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"type\", StringType(), nullable=True),\n",
        "    StructField(\"id\", StringType(), nullable=True),\n",
        "    StructField(\"sid\", StringType(), nullable=True),\n",
        "    StructField(\"sname\", StringType(), nullable=True),\n",
        "    StructField(\"snsfw\", StringType(), nullable=True),\n",
        "    StructField(\"created_utc\", IntegerType(), nullable=True),\n",
        "    StructField(\"permalink\", StringType(), nullable=True),\n",
        "    StructField(\"body\", StringType(), nullable=True),\n",
        "    StructField(\"sentiment\", FloatType(), nullable=True),\n",
        "    StructField(\"score\", IntegerType(), nullable=True)\n",
        "])\n",
        "# Convert Pandas DataFrame to Spark DataFrame in chunks\n",
        "chunk_size = 10000\n",
        "num_chunks = len(df) // chunk_size + 1\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    # Convert Pandas DataFrame chunk to Spark DataFrame\n",
        "    spark_df_chunk = spark.createDataFrame(chunk, schema=schema)\n",
        "    new_df = spark_df_chunk.select(\"body\")\n",
        "    # Apply the same preprocessing steps to the chunk\n",
        "    chunk_transformed = pipelineFit.transform(new_df)\n",
        "    \n",
        "    # Make predictions using the logistic regression model\n",
        "    predictions = lrModel.transform(chunk_transformed)\n",
        "    \n",
        "    # Select the 'body' and 'prediction' columns from the predictions\n",
        "    body_predictions = predictions.select(\"body\", \"prediction\")\n",
        "    \n",
        "    # Convert the Spark DataFrame back to Pandas DataFrame\n",
        "    body_predictions_pandas = body_predictions.toPandas()\n",
        "    \n",
        "    return body_predictions_pandas\n",
        "\n",
        "# Process each chunk and store the results\n",
        "results = []\n",
        "for i in range(num_chunks):\n",
        "    start = i * chunk_size\n",
        "    end = start + chunk_size\n",
        "    chunk = df.iloc[start:end]\n",
        "    chunk_predictions = process_chunk(chunk)\n",
        "    results.append(chunk_predictions)\n",
        "\n",
        "# Concatenate the results\n",
        "body_predictions_pandas = pd.concat(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVhKR3bfYekz"
      },
      "outputs": [],
      "source": [
        "merged_df = df.merge(body_predictions_pandas, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "merged_df = merged_df.drop('body_y', axis=1)\n",
        "# Remove duplicate values from 'body_x' column\n",
        "merged_df['body_x'] = merged_df['body_x'].drop_duplicates()\n",
        "\n",
        "# Rename 'body_x' column to 'body'\n",
        "merged_df = merged_df.rename(columns={'body_x': 'body'})\n",
        "\n",
        "merged_df = merged_df.drop_duplicates(subset='body', keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl1tN3SQU7rD"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_QfAFIafFUS",
        "outputId": "328d52f9-f668-4ae3-f8e2-5d40a52f4738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table created: famous-athlete-386604.reddit.comment\n"
          ]
        }
      ],
      "source": [
        "project_id = 'replace with your project id'\n",
        "dataset_name = 'reddit'\n",
        "table_name = 'comment'\n",
        "\n",
        "# Create a BigQuery client\n",
        "bigquery_client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Create the dataset reference\n",
        "dataset_ref = bigquery_client.dataset(dataset_name)\n",
        "\n",
        "# Create the dataset if it doesn't exist\n",
        "if not bigquery_client.get_dataset(dataset_ref):\n",
        "    bigquery_client.create_dataset(dataset_ref)\n",
        "\n",
        "# Define the table schema\n",
        "schema = [\n",
        "\n",
        "    bigquery.SchemaField(\"type\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"id\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"sid\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"sname\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"snsfw\", \"BOOL\"),\n",
        "    bigquery.SchemaField(\"created_utc\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"permalink\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"body\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"sentiment\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"score\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"prediction\", \"FLOAT\"),\n",
        "]\n",
        "\n",
        "# Create the table reference\n",
        "table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "# Define the table configuration\n",
        "table = bigquery.Table(table_ref, schema=schema)\n",
        "\n",
        "# Create the table\n",
        "table = bigquery_client.create_table(table)\n",
        "\n",
        "print(f'Table created: {project_id}.{dataset_name}.{table_name}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33NwXaEAZecP"
      },
      "outputs": [],
      "source": [
        "# Write the DataFrame to the BigQuery table\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job = bigquery_client.load_table_from_dataframe(merged_df, table_ref, job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f'Data uploaded to BigQuery table: {project_id}.{dataset_name}.{table_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rjjc1KGfYnB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn_K-iHEfYqY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
